# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UCxSBP2w0lpZcxIfFMgLOCcGwRlICk72

# **Preparing Data**
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d 'praveengovi/emotions-dataset-for-nlp'

import zipfile

dataset_zip = zipfile.ZipFile('emotions-dataset-for-nlp.zip')

dataset_zip.extractall()

dataset_zip.close()

import re
import tqdm
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""# **Reading Data**"""

train = pd.read_csv('train.txt')
tes = pd.read_csv('test.txt')
val = pd.read_csv('val.txt')

train.head()

train = pd.read_csv('train.txt', sep=';', header=None, names=['text', 'label'])
test = pd.read_csv('test.txt', sep=';', header=None, names=['text', 'label'])
val = pd.read_csv('val.txt', sep=';', header=None, names=['text', 'label'])

train.head()

"""# **Get Info From Dataset**"""

print("Train Shape", train.shape)
print("Test Shape", test.shape)
print("Val Shape", val.shape)

print("Train")
train.isnull().sum()

print("Test")
test.isnull().sum()

print("Val")
val.isnull().sum()

plt.hist(train['label'], bins=12)
plt.xlabel('Label')
plt.ylabel('Frekuensi')
plt.title('Distribusi Train')
plt.show()

plt.hist(test['label'], bins=12)
plt.xlabel('Label')
plt.ylabel('Frekuensi')
plt.title('Distribusi Test')
plt.show()

plt.hist(val['label'], bins=12)
plt.xlabel('Label')
plt.ylabel('Frekuensi')
plt.title('Distribusi Validation')
plt.show()

sample_train = train['text'][20]
sample_test = test['text'][20]
sample_val = val['text'][20]

print("train sample txt:", sample_train)
print("test sample txt:", sample_test)
print("val sample txt:", sample_val)

"""# **Data Cleaning**

**1. Lowercase**

**2. Removing number**

**3. Removing Punctuation**

**4. Remove Stopwords**

**5. Lemmatize**
"""

!pip install contractions

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import contractions

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()
stopwords = stopwords.words('english')

def lower_text(text):
    return text.lower()

def remove_number(text):
    num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
    return num.sub(r'', text)

def remove_punct(text):
    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + "'`"
    for p in punctuations:
        text = text.replace(p, f' {p} ')
    return text

def remove_stopwords(text):
    text = ' '.join([word for word in text.split() if word not in (stopwords)])
    return text

def lemmatize_lower(text):
    stop_words = [word for word in text.split() if word not in (stopwords)]
    text = ' '.join([lemmatizer.lemmatize(contractions.fix(lower_text(text))) for txt in text.split() if txt not in stop_words])
    return text

def clean_text(text):
    text = remove_number(text)
    text = remove_punct(text)
    text = lemmatize_lower(text)
    text = remove_stopwords(text)

    return text

train.head(10)

train["clean_text"] = train["text"].apply(clean_text)
test["clean_text"] = test["text"].apply(clean_text)
val["clean_text"] = val["text"].apply(clean_text)

train.head(10)

"""# **Convert Label to Num**"""

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

train["encoded_label"] = encoder.fit_transform(train["label"])
test["encoded_label"] = encoder.transform(test["label"])
val["encoded_label"] = encoder.transform(val["label"])

X_train = train['clean_text']
X_test = val['clean_text']
y_train = train['encoded_label']
y_test = val['encoded_label']

train.head(10)

"""# **Tokenization**"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

import tensorflow as tf

tokenizer = Tokenizer(oov_token='')
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
X_val = tokenizer.texts_to_sequences(test['clean_text'])

vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)

X_train = pad_sequences(X_train, padding='post', maxlen=128)
X_test = pad_sequences(X_test, padding='post', maxlen=128)
X_val = pad_sequences(X_val, padding='post', maxlen=128)

"""# **One Hot Encoding**"""

dmy_y_train = to_categorical(y_train)
 dmy_y_test = to_categorical(y_test)

"""# **Model Development**"""

from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

model = Sequential([
    Embedding(vocab_size, 64, input_length=128),
    Dropout(0.4),
    Bidirectional(LSTM(128, return_sequences=True)),
    Bidirectional(LSTM(256)),
    Dense(6, activation='softmax')
])

model.summary()

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90 and logs.get('val_accuracy')>0.85):
      print("\nAccuracy above 90%, finish training!")
      self.model.stop_training = True

callbacks = myCallback()

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train, dmy_y_train,
                    epochs=200,
                    verbose=1,
                    validation_data=(X_test, dmy_y_test),
                    batch_size=128,
                    callbacks = [callbacks])

"""# **Save Model**"""

import pathlib
export_dir = 'saved_model/LSTM'
tf.saved_model.save(model, export_dir)

"""# **Plot Training**"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'], loc = 'upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

predict = model.predict(X_val)
predict_class = np.argmax(predict, axis=1)
predict_class = np.array(predict_class)
predict_class

"""# **Evaluation**"""

from sklearn.metrics import classification_report, accuracy_score
print("\nClassification Report:\n",classification_report(test["encoded_label"], predict_class, target_names =["anger", "fear", "joy", "love", "sadness", "surprise"]))

